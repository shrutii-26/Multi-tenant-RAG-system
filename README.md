Multi-Tenant Retrieval-Augmented Generation (RAG) System

A full-stack AI system that allows users to upload their own PDF documents and query them using Retrieval-Augmented Generation (RAG).

Each upload creates an isolated knowledge base with dynamic FAISS indexing, enabling per-user document querying.

.

ğŸš€ Features

Dynamic PDF upload

Automatic text extraction and chunking

Per-upload FAISS vector index creation

Retrieval-based context grounding

LLM-based answer generation (Groq)

Source citation display

Knowledge base isolation using unique IDs

Full-stack deployment (FastAPI + React)

ğŸ— Architecture
Backend

FastAPI â€“ REST API layer

FAISS â€“ Vector similarity search

SentenceTransformers (all-MiniLM-L6-v2) â€“ Embeddings

Groq LLM API â€“ Answer generation

PyPDF2 â€“ PDF text extraction

Frontend

React (Vite) â€“ UI layer

REST API integration

Dynamic upload and query interface

ğŸ”„ System Flow

User uploads one or more PDF documents.

Backend extracts text using PyPDF2.

Text is split into overlapping chunks.

Embeddings are generated using SentenceTransformers.

A FAISS index is built and stored per unique knowledge base ID.

User submits a question.

Top-k relevant chunks are retrieved from FAISS.

Retrieved context is sent to the LLM.

Grounded answer + sources are returned to the frontend.

ğŸ“‚ Project Structure
comparative-rag-project/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ ingestion.py
â”‚ â”œâ”€â”€ retrieval.py
â”‚ â”œâ”€â”€ generator.py
â”‚ â””â”€â”€ config.py
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ src/App.jsx
â”‚
â”œâ”€â”€ indexes/ # auto-generated (ignored in git)
â”œâ”€â”€ .env # environment variables (ignored in git)
â””â”€â”€ README.md

ğŸ” Environment Variables

Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here

ğŸ›  Local Development
1ï¸âƒ£ Backend
uvicorn backend.main:app --reload

Runs at:

http://127.0.0.1:8000

Swagger docs:

http://127.0.0.1:8000/docs

2ï¸âƒ£ Frontend
cd frontend
npm install
npm run dev

Runs at:

http://localhost:5173

ğŸŒ Deployment

Backend

Deploy to Render

Start command:

uvicorn backend.main:app --host 0.0.0.0 --port 10000

Frontend

Deploy frontend/ directory to Vercel

Update API URLs to deployed backend URL

ğŸ“Œ Why This Project Matters

This project demonstrates:

Multi-tenant AI system design

Retrieval-Augmented Generation (RAG)

Vector search implementation using FAISS

Dynamic knowledge base isolation

Production-ready REST API architecture

Full-stack AI deployment

Error handling and clean API responses

âš™ï¸ Future Improvements

User authentication layer

Persistent cloud storage (S3 / database)

Streaming LLM responses

Better UI/UX design

Rate limiting and usage controls

ğŸ‘©â€ğŸ’» Author

Built as a production-style AI engineering project demonstrating full-stack RAG deployment.
